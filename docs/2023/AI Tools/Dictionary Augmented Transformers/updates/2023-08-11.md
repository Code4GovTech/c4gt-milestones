---
title: Week 6
author: Ishaan Shrivastava
---

## Milestones
- [x] Experiment 1: Visual Inspection / Comparison of Tesseract OCR versus new CLIP approach on classifying language
- [x] Experiment 2:
    - [x] prompt tuning - found best prompt - "image of odiya/english language text"
    - [x] test other CLIP models - best model found - ViT-B/16
    - [ ] try setting a threshold parameter that is learnt automatically on the dataset
    - [ ] draw confusion matrices comparing the errors on CLIP and Tesseract classification
- [x] Set up a git repository - [shrivastava95/clip-ocr](https://github.com/shrivastava95/clip-ocr) for fine-tuning CLIP onto a given dataset.
    - [x] Created a dataset of cropped word images from some pages of [en-or.pdf](https://github.com/shrivastava95/odia-dictionary/blob/main/en-or.pdf)
    - [x] Implemented base zero-shot approach
    - [ ] ~~Implemented finetuning approach - too costly, skipped~~
    - [x] Implemented CoOp - https://arxiv.org/abs/2109.01134 as a cheaper alternative to finetuning CLIP
    - [ ] Implemented CoCoOp - https://arxiv.org/abs/2203.05557 as a more generalisable approach than CoCoOp
 
## Screenshots / Videos 
- image of results of CoOp on the handcrafted dataset comprising 1000+ images
![image of results of CoOp on the handcrafted dataset comprising 1000+ images](https://i.imgur.com/BHc7ZIQ.png)

## Contributions
- https://github.com/shrivastava95/clip-ocr/pull/2

## Learnings
- Learnt to use CLIP as an effective tool for Zero-Shot image-text tasks.
- Read and Implemented the [CoOp](https://arxiv.org/abs/2109.01134) and [CoCoOp](https://arxiv.org/abs/2203.05557) papers from scratch.